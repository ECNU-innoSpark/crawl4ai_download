import asyncio
import argparse
import os
import json
import re
from pathlib import Path
from urllib.parse import urljoin, urlparse
import aiohttp
from crawl4ai import AsyncWebCrawler


def extract_year_from_url(url: str) -> str:
    """
    ä» URL ä¸­æå–å¹´ä»½
    ä¾‹å¦‚: /paper_files/paper/2024/file/xxx.pdf -> 2024
    """
    match = re.search(r'/(\d{4})/', url)
    return match.group(1) if match else "unknown"


async def download_file(url: str, save_path: str, session: aiohttp.ClientSession, year: str = None):
    """
    ä¸‹è½½å•ä¸ªæ–‡ä»¶
    """
    try:
        async with session.get(url, timeout=aiohttp.ClientTimeout(total=60)) as response:
            if response.status == 200:
                # ä» URL ä¸­æå–æ–‡ä»¶å
                filename = os.path.basename(urlparse(url).path)
                if not filename:
                    filename = f"downloaded_{hash(url)}.pdf"
                
                # å¦‚æœæŒ‡å®šäº†å¹´ä»½ï¼Œåˆ›å»ºå¹´ä»½å­ç›®å½•
                if year:
                    year_dir = os.path.join(save_path, year)
                    Path(year_dir).mkdir(parents=True, exist_ok=True)
                    filepath = os.path.join(year_dir, filename)
                else:
                    filepath = os.path.join(save_path, filename)
                
                # æ£€æŸ¥æ–‡ä»¶æ˜¯å¦å·²å­˜åœ¨
                if os.path.exists(filepath):
                    file_size = os.path.getsize(filepath)
                    if year:
                        print(f"â­ï¸  å·²å­˜åœ¨: {year}/{filename} ({file_size / 1024:.2f} KB)")
                    else:
                        print(f"â­ï¸  å·²å­˜åœ¨: {filename} ({file_size / 1024:.2f} KB)")
                    return filepath
                
                # ä¿å­˜æ–‡ä»¶
                with open(filepath, 'wb') as f:
                    f.write(await response.read())
                
                file_size = os.path.getsize(filepath)
                if year:
                    print(f"âœ… ä¸‹è½½æˆåŠŸ: {year}/{filename} ({file_size / 1024:.2f} KB)")
                else:
                    print(f"âœ… ä¸‹è½½æˆåŠŸ: {filename} ({file_size / 1024:.2f} KB)")
                return filepath
            else:
                print(f"âŒ ä¸‹è½½å¤±è´¥: {url} (çŠ¶æ€ç : {response.status})")
                return None
    except Exception as e:
        print(f"âŒ ä¸‹è½½å‡ºé”™: {url} - {str(e)}")
        return None


async def crawl_and_download_pdfs(url: str, output_dir: str = "downloaded_pdfs", max_concurrent: int = 5):
    """
    çˆ¬å–ç½‘é¡µå¹¶ä¸‹è½½æ‰€æœ‰ PDF æ–‡ä»¶
    
    Args:
        url: è¦çˆ¬å–çš„ç½‘é¡µ URL
        output_dir: PDF æ–‡ä»¶ä¿å­˜ç›®å½•
        max_concurrent: æœ€å¤§å¹¶å‘ä¸‹è½½æ•° (é»˜è®¤: 5)
    """
    # åˆ›å»ºè¾“å‡ºç›®å½•
    Path(output_dir).mkdir(parents=True, exist_ok=True)
    
    print(f"ğŸš€ å¼€å§‹çˆ¬å–ç½‘é¡µ: {url}")
    
    # åˆå§‹åŒ–çˆ¬è™«
    async with AsyncWebCrawler(verbose=False) as crawler:
        # çˆ¬å–ç½‘é¡µ
        result = await crawler.arun(
            url=url,
            bypass_cache=True,
        )
        
        if not result.success:
            print(f"âŒ çˆ¬å–å¤±è´¥: {result.error_message}")
            return
        
        print(f"âœ… ç½‘é¡µçˆ¬å–æˆåŠŸ")
        
        # æå–æ‰€æœ‰é“¾æ¥
        if not hasattr(result, 'links') or not result.links:
            print("âš ï¸  æœªæ‰¾åˆ°ä»»ä½•é“¾æ¥ï¼Œå°è¯•ä»å†…å®¹ä¸­æå– PDF é“¾æ¥...")
            # ä» HTML ä¸­æ‰‹åŠ¨æå– PDF é“¾æ¥
            import re
            pdf_pattern = r'href=["\']([^"\']*\.pdf[^"\']*)["\']'
            pdf_links = re.findall(pdf_pattern, result.html, re.IGNORECASE)
        else:
            # ç­›é€‰ PDF é“¾æ¥ - result.links è¿”å›çš„æ˜¯å­—å…¸åˆ—è¡¨ï¼Œæ¯ä¸ªå­—å…¸åŒ…å« href å’Œ text
            all_links = result.links.get('external', []) + result.links.get('internal', [])
            pdf_links = []
            for link in all_links:
                if isinstance(link, dict):
                    href = link.get('href', '')
                    if href and href.lower().endswith('.pdf'):
                        pdf_links.append(href)
                elif isinstance(link, str) and link.lower().endswith('.pdf'):
                    pdf_links.append(link)
        
        if not pdf_links:
            print("âŒ æœªæ‰¾åˆ°ä»»ä½• PDF æ–‡ä»¶é“¾æ¥")
            return
        
        # è½¬æ¢ä¸ºç»å¯¹ URL
        absolute_pdf_links = []
        for link in pdf_links:
            if link.startswith('http://') or link.startswith('https://'):
                absolute_pdf_links.append(link)
            else:
                absolute_pdf_links.append(urljoin(url, link))
        
        # å»é‡
        absolute_pdf_links = list(set(absolute_pdf_links))
        
        print(f"ğŸ“„ æ‰¾åˆ° {len(absolute_pdf_links)} ä¸ª PDF æ–‡ä»¶:")
        for i, link in enumerate(absolute_pdf_links, 1):
            print(f"  {i}. {link}")
        
        # ç»Ÿè®¡å¹´ä»½åˆ†å¸ƒ
        year_count = {}
        for link in absolute_pdf_links:
            year = extract_year_from_url(link)
            year_count[year] = year_count.get(year, 0) + 1
        
        print(f"\nğŸ“Š å¹´ä»½åˆ†å¸ƒ:")
        for year in sorted(year_count.keys()):
            print(f"  {year}: {year_count[year]} ä¸ªæ–‡ä»¶")
        
        print(f"\nâ¬‡ï¸  å¼€å§‹ä¸‹è½½åˆ°ç›®å½•: {os.path.abspath(output_dir)}")
        print(f"âš™ï¸  å¹¶å‘æ•°: {max_concurrent}")
        
        # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘æ•°
        semaphore = asyncio.Semaphore(max_concurrent)
        
        async def download_with_semaphore(pdf_url):
            async with semaphore:
                async with aiohttp.ClientSession() as session:
                    return await download_file(pdf_url, output_dir, session, extract_year_from_url(pdf_url))
        
        # å¼‚æ­¥ä¸‹è½½æ‰€æœ‰ PDF
        tasks = [download_with_semaphore(pdf_url) for pdf_url in absolute_pdf_links]
        results = await asyncio.gather(*tasks)
        
        # ç»Ÿè®¡ä¸‹è½½ç»“æœ
        successful = sum(1 for r in results if r is not None)
        print(f"\nğŸ‰ ä¸‹è½½å®Œæˆ! æˆåŠŸ: {successful}/{len(absolute_pdf_links)}")


async def process_jsonl(jsonl_path: str, output_dir: str = "downloaded_pdfs", url_field: str = 'url', max_concurrent: int = 20):
    """
    ä» JSONL æ–‡ä»¶è¯»å– URL åˆ—è¡¨ï¼Œå¹¶å‘çˆ¬å–å’Œä¸‹è½½
    
    Args:
        jsonl_path: JSONL æ–‡ä»¶è·¯å¾„
        output_dir: PDF æ–‡ä»¶ä¿å­˜ç›®å½•
        url_field: JSONL ä¸­ URL å­—æ®µå
        max_concurrent: æœ€å¤§å¹¶å‘çˆ¬å–URLæ•° (é»˜è®¤: 20)
    """
    print(f"{'='*80}")
    print(f"ğŸ“‚ è¯»å– JSONL æ–‡ä»¶: {jsonl_path}")
    print(f"{'='*80}\n")
    
    # è¯»å– JSONL æ–‡ä»¶ä¸­çš„ URL
    urls = []
    with open(jsonl_path, 'r', encoding='utf-8') as f:
        for line_num, line in enumerate(f, 1):
            line = line.strip()
            if not line:
                continue
            try:
                data = json.loads(line)
                url = data.get(url_field)
                if url:
                    urls.append(url)
            except json.JSONDecodeError as e:
                print(f"âš ï¸  è·³è¿‡ç¬¬ {line_num} è¡Œ (JSON è§£æå¤±è´¥): {str(e)[:50]}")
    
    if not urls:
        print("âŒ æ²¡æœ‰æ‰¾åˆ°ä»»ä½• URL")
        return
    
    print(f"âœ… è¯»å–åˆ° {len(urls)} ä¸ª URL")
    print(f"âš™ï¸  å¹¶å‘çˆ¬å–æ•°: {max_concurrent}")
    print(f"{'='*80}\n")
    
    # ä½¿ç”¨ä¿¡å·é‡æ§åˆ¶å¹¶å‘URLçˆ¬å–æ•°
    semaphore = asyncio.Semaphore(max_concurrent)
    total_processed = 0
    total_urls = len(urls)
    
    async def crawl_with_semaphore(url, idx):
        nonlocal total_processed
        async with semaphore:
            print(f"\n{'â”€'*80}")
            print(f"[{idx}/{total_urls}] çˆ¬å–: {url[:70]}...")
            print(f"{'â”€'*80}")
            await crawl_and_download_pdfs(url, output_dir, max_concurrent=10)
            total_processed += 1
            print(f"âœ“ å·²å®Œæˆ {total_processed}/{total_urls}")
    
    # å¹¶å‘çˆ¬å–æ‰€æœ‰ URL
    tasks = [crawl_with_semaphore(url, idx) for idx, url in enumerate(urls, 1)]
    await asyncio.gather(*tasks, return_exceptions=True)
    
    print(f"\n{'='*80}")
    print(f"ğŸ‰ å…¨éƒ¨å®Œæˆ! å·²å¤„ç† {total_urls} ä¸ª URL")
    print(f"{'='*80}")


def main():
    parser = argparse.ArgumentParser(
        description="ä»ç½‘é¡µæˆ– JSONL æ–‡ä»¶çˆ¬å–å¹¶ä¸‹è½½æ‰€æœ‰ PDF æ–‡ä»¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ç¤ºä¾‹:
  # ä» JSONL æ–‡ä»¶è¯»å– URL åˆ—è¡¨ï¼ˆé»˜è®¤å¹¶å‘20ä¸ªURLï¼‰
  python download_pdf.py --jsonl config1_level2.jsonl
  
  # ä»å•ä¸ªç½‘é¡µçˆ¬å–
  python download_pdf.py --url https://example.com/papers
  
  # æŒ‡å®šè¾“å‡ºç›®å½•å’Œå¹¶å‘çˆ¬å–æ•°
  python download_pdf.py --jsonl urls.jsonl --output my_pdfs --max-concurrent 10
  
  # é«˜å¹¶å‘çˆ¬å–ï¼ˆåŒæ—¶å¤„ç†50ä¸ªURLï¼‰
  python download_pdf.py --jsonl urls.jsonl --max-concurrent 50
  
æ³¨æ„:
  - max-concurrent æ§åˆ¶åŒæ—¶çˆ¬å–çš„URLæ•°é‡
  - æ¯ä¸ªURLçˆ¬å–åˆ°çš„PDFæ–‡ä»¶å†…éƒ¨å¹¶å‘10ä¸ªä¸‹è½½
  - å»ºè®®æ ¹æ®ç½‘ç»œçŠ¶å†µå’ŒæœåŠ¡å™¨æ€§èƒ½è°ƒæ•´å¹¶å‘æ•°
        """
    )
    parser.add_argument(
        "--jsonl",
       default="config1_level2.jsonl",
        help="JSONL æ–‡ä»¶è·¯å¾„ï¼ˆæ¯è¡Œä¸€ä¸ªåŒ…å« URL çš„ JSON å¯¹è±¡ï¼‰"
    )
    parser.add_argument(
        "--url-field",
        type=str,
        default="url",
        help="JSONL ä¸­ URL å­—æ®µå (é»˜è®¤: url)"
    )
    parser.add_argument(
        "--url",
        type=str,
        help="è¦çˆ¬å–çš„ç½‘é¡µ URLï¼ˆå•ä¸ª URL æ¨¡å¼ï¼‰"
    )
    parser.add_argument(
        "--output",
        type=str,
        default="downloaded_pdfs",
        help="PDF æ–‡ä»¶ä¿å­˜ç›®å½• (é»˜è®¤: downloaded_pdfs)"
    )
    parser.add_argument(
        "--max-concurrent",
        type=int,
        default=20,
        help="æœ€å¤§å¹¶å‘çˆ¬å–URLæ•° (é»˜è®¤: 20ï¼Œæ¯ä¸ªURLä¸‹è½½PDFæ—¶å†…éƒ¨å¹¶å‘10ä¸ª)"
    )
    
    args = parser.parse_args()
    
    # åˆ¤æ–­ä½¿ç”¨å“ªç§æ¨¡å¼
    if args.jsonl:
        # JSONL æ¨¡å¼ï¼šä»æ–‡ä»¶è¯»å– URL åˆ—è¡¨
        asyncio.run(process_jsonl(args.jsonl, args.output, args.url_field, args.max_concurrent))
    elif args.url:
        # å•ä¸ª URL æ¨¡å¼
        asyncio.run(crawl_and_download_pdfs(args.url, args.output, args.max_concurrent))
    else:
        parser.print_help()
        print("\nâŒ é”™è¯¯: å¿…é¡»æŒ‡å®š --jsonl æˆ– --url å‚æ•°")


if __name__ == "__main__":
    main()
