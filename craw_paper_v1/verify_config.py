"""
éªŒè¯é…ç½®æ–‡ä»¶çš„æ­£åˆ™è¡¨è¾¾å¼æ˜¯å¦æ­£ç¡®
"""
import asyncio
import json
import re
import time
from pathlib import Path
from typing import Dict, Any, List
from urllib.parse import urljoin
import yaml


class ConfigValidator:
    """é…ç½®éªŒè¯å™¨"""
    
    def __init__(self, config_path: str):
        self.config_path = Path(config_path)
        self.config = self._load_config()
        
    def _load_config(self) -> Dict[str, Any]:
        """åŠ è½½é…ç½®æ–‡ä»¶"""
        if not self.config_path.exists():
            raise FileNotFoundError(f"é…ç½®æ–‡ä»¶ä¸å­˜åœ¨: {self.config_path}")
        
        with open(self.config_path, 'r', encoding='utf-8') as f:
            return yaml.safe_load(f)
    
    async def validate_and_extract(self):
        """éªŒè¯é…ç½®å¹¶æå–å„å±‚çº§URL"""
        levels = self.config.get('target', {}).get('levels', [])
        base_url = self.config.get('target', {}).get('base_url', 'https://papers.nips.cc/')
        
        if not levels:
            print("âŒ é…ç½®ä¸­æ²¡æœ‰æ‰¾åˆ° levels")
            return
        
        print(f"{'='*80}")
        print(f"å¼€å§‹éªŒè¯é…ç½®: {self.config_path}")
        print(f"åŸºç¡€URL: {base_url}")
        print(f"å±‚çº§æ•°é‡: {len(levels)}")
        print(f"âš ï¸  æ³¨æ„: å°†å¤„ç†å’Œä¿å­˜æ‰€æœ‰æå–åˆ°çš„URLï¼Œå¯èƒ½éœ€è¦è¾ƒé•¿æ—¶é—´")
        print(f"{'='*80}\n")
        
        # ä»ç¬¬ä¸€å±‚å¼€å§‹
        current_urls = [base_url]
        all_results = {}
        
        try:
            from crawl4ai import AsyncWebCrawler
            
            async with AsyncWebCrawler(verbose=False) as crawler:
                for level_config in levels:
                    level = level_config.get('level')
                    level_name = level_config.get('name', f'Level{level}')
                    extract_pattern = level_config.get('extract_pattern', '')
                    filter_pattern = level_config.get('filter_pattern', '')
                    url_pattern = level_config.get('url_pattern', '')
                    description = level_config.get('description', '')
                    
                    print(f"\n{'â”€'*80}")
                    print(f"ğŸ“Š å±‚çº§ {level}: {level_name}")
                    print(f"{'â”€'*80}")
                    print(f"URLåŒ¹é…æ¨¡å¼: {url_pattern}")
                    print(f"æå–æ¨¡å¼:    {extract_pattern}")
                    print(f"è¿‡æ»¤æ¨¡å¼:    {filter_pattern}")
                    print(f"è¯´æ˜:        {description}")
                    print(f"\nå½“å‰å¾…å¤„ç†URLæ•°: {len(current_urls)}")
                    
                    level_results = []
                    next_urls = []
                    extracted_count = 0
                    filtered_count = 0
                    
                    # å¤„ç†æ‰€æœ‰URL
                    total_to_process = len(current_urls)
                    estimated_time = total_to_process * 0.5 / 60  # ä¼°è®¡æ—¶é—´ï¼ˆåˆ†é’Ÿï¼‰
                    print(f"  å¼€å§‹å¤„ç† {total_to_process} ä¸ªURL (é¢„è®¡éœ€è¦çº¦ {estimated_time:.1f} åˆ†é’Ÿ)...")
                    
                    start_time = time.time()
                    
                    for idx, source_url in enumerate(current_urls, 1):
                        # æ¯10ä¸ªURLæ˜¾ç¤ºä¸€æ¬¡è¿›åº¦
                        if idx % 10 == 1 or len(current_urls) <= 10:
                            print(f"\n  [{idx}/{len(current_urls)}] çˆ¬å–: {source_url[:80]}...")
                        
                        try:
                            result = await crawler.arun(url=source_url, bypass_cache=True)
                            
                            if result.success:
                                # ä½¿ç”¨æå–æ¨¡å¼æå–é“¾æ¥
                                if extract_pattern:
                                    raw_links = re.findall(extract_pattern, result.html, re.IGNORECASE)
                                    extracted_count += len(raw_links)
                                    
                                    if idx % 10 == 1 or len(current_urls) <= 10:
                                        print(f"      âœ“ æå–åˆ° {len(raw_links)} ä¸ªé“¾æ¥")
                                    
                                    # å»é‡
                                    raw_links = list(set(raw_links))
                                    
                                    # è½¬æ¢ä¸ºç»å¯¹URLå¹¶åº”ç”¨è¿‡æ»¤
                                    for link in raw_links:
                                        # å¤„ç†ç›¸å¯¹è·¯å¾„
                                        if link.startswith('http'):
                                            full_url = link
                                        elif link.startswith('/'):
                                            full_url = urljoin(base_url, link)
                                        else:
                                            full_url = urljoin(source_url, link)
                                        
                                        # åº”ç”¨è¿‡æ»¤è§„åˆ™
                                        if filter_pattern:
                                            # å°è¯•åŒ¹é…å®Œæ•´URLæˆ–ç›¸å¯¹è·¯å¾„
                                            if re.match(filter_pattern, full_url) or re.match(filter_pattern, link):
                                                filtered_count += 1
                                                
                                                # ä¿å­˜ç»“æœ
                                                level_results.append({
                                                    'level': level,
                                                    'level_name': level_name,
                                                    'url': full_url,
                                                    'source_url': source_url,
                                                    'extract_pattern': extract_pattern,
                                                    'filter_pattern': filter_pattern,
                                                    'matched_text': link
                                                })
                                                
                                                # ä¼ é€’ç»™ä¸‹ä¸€å±‚ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
                                                next_urls.append(full_url)
                                        else:
                                            # æ²¡æœ‰è¿‡æ»¤è§„åˆ™ï¼Œå…¨éƒ¨ä¿ç•™
                                            filtered_count += 1
                                            level_results.append({
                                                'level': level,
                                                'level_name': level_name,
                                                'url': full_url,
                                                'source_url': source_url,
                                                'extract_pattern': extract_pattern,
                                                'filter_pattern': filter_pattern,
                                                'matched_text': link
                                            })
                                            
                                            # ä¼ é€’ç»™ä¸‹ä¸€å±‚ï¼ˆä¸é™åˆ¶æ•°é‡ï¼‰
                                            next_urls.append(full_url)
                                else:
                                    if idx % 10 == 1 or len(current_urls) <= 10:
                                        print(f"      âš  æ²¡æœ‰æå–æ¨¡å¼ï¼Œè·³è¿‡")
                            else:
                                if idx % 10 == 1 or len(current_urls) <= 10:
                                    print(f"      âœ— çˆ¬å–å¤±è´¥")
                            
                            # å»¶è¿Ÿé¿å…è¯·æ±‚è¿‡å¿«
                            await asyncio.sleep(0.5)
                            
                        except Exception as e:
                            if idx % 10 == 1 or len(current_urls) <= 10:
                                print(f"      âœ— é”™è¯¯: {str(e)[:50]}")
                    
                    # å»é‡ï¼šæŒ‰URLå»é‡ï¼Œä¿ç•™ç¬¬ä¸€æ¬¡å‡ºç°çš„è®°å½•
                    seen_urls = set()
                    deduplicated_results = []
                    for item in level_results:
                        if item['url'] not in seen_urls:
                            seen_urls.add(item['url'])
                            deduplicated_results.append(item)
                    level_results = deduplicated_results
                    
                    # å»é‡ä¸‹ä¸€å±‚URL
                    next_urls = list(set(next_urls))
                    
                    elapsed_time = time.time() - start_time
                    
                    print(f"\n  âœ… å±‚çº§ {level} å¤„ç†å®Œæˆ (ç”¨æ—¶ {elapsed_time/60:.1f} åˆ†é’Ÿ)")
                    print(f"  ç»Ÿè®¡:")
                    print(f"    - å¤„ç†çš„æºURLæ•°:      {len(current_urls)}")
                    print(f"    - æå–åˆ°çš„åŸå§‹é“¾æ¥æ•°: {extracted_count}")
                    print(f"    - è¿‡æ»¤åçš„é“¾æ¥æ•°:     {filtered_count}")
                    print(f"    - å»é‡åä¿ç•™é“¾æ¥æ•°:   {len(level_results)}")
                    print(f"    - ä¼ é€’ç»™ä¸‹ä¸€å±‚æ•°:     {len(next_urls)}")
                    
                    # ä¿å­˜å½“å‰å±‚çº§ç»“æœ
                    all_results[f'level_{level}'] = level_results
                    
                    # ä¿å­˜åˆ°JSONL
                    output_file = f"{self.config_path.stem}_level{level}.jsonl"
                    self._save_jsonl(output_file, level_results)
                    print(f"    - å·²ä¿å­˜åˆ°: {output_file}")
                    
                    # æ›´æ–°ä¸‹ä¸€å±‚çš„URL
                    current_urls = next_urls
                    
                    # å¦‚æœæ²¡æœ‰æ›´å¤šURLï¼Œåœæ­¢
                    if not current_urls:
                        print(f"\n  âš ï¸ å±‚çº§ {level} åæ— æ›´å¤šé“¾æ¥ï¼ŒéªŒè¯ç»“æŸ")
                        break
                
                # è¾“å‡ºæ€»ç»“
                self._print_summary(all_results)
                
        except ImportError:
            print("âŒ é”™è¯¯: éœ€è¦å®‰è£… crawl4ai")
            print("   pip install crawl4ai")
        except Exception as e:
            print(f"âŒ éªŒè¯è¿‡ç¨‹å‡ºé”™: {e}")
            import traceback
            traceback.print_exc()
    
    def _save_jsonl(self, filename: str, data: List[Dict]):
        """ä¿å­˜åˆ°JSONLæ–‡ä»¶"""
        with open(filename, 'w', encoding='utf-8') as f:
            for item in data:
                json.dump(item, f, ensure_ascii=False)
                f.write('\n')
    
    def _print_summary(self, all_results: Dict[str, List]):
        """æ‰“å°æ€»ç»“"""
        print(f"\n{'='*80}")
        print(f"éªŒè¯å®Œæˆï¼")
        print(f"{'='*80}")
        
        total_urls = 0
        for level_key, results in all_results.items():
            level_num = level_key.split('_')[1]
            level_name = results[0]['level_name'] if results else 'æœªçŸ¥'
            count = len(results)
            total_urls += count
            
            print(f"  å±‚çº§ {level_num} ({level_name}): {count} ä¸ªURL")
        
        print(f"\n  æ€»è®¡: {total_urls} ä¸ªURL")
        print(f"{'='*80}\n")


async def main():
    """ä¸»å‡½æ•°"""
    import argparse
    
    parser = argparse.ArgumentParser(
        description="éªŒè¯çˆ¬è™«é…ç½®æ–‡ä»¶",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
ä½¿ç”¨ç¤ºä¾‹:
  # éªŒè¯é…ç½®æ–‡ä»¶
  python verify_config.py config1.yaml
  
  # éªŒè¯é»˜è®¤é…ç½®
  python verify_config.py
        """
    )
    
    parser.add_argument(
        'config_file',
        nargs='?',
        default='config1.yaml',
        help='é…ç½®æ–‡ä»¶è·¯å¾„ (é»˜è®¤: config1.yaml)'
    )
    
    args = parser.parse_args()
    
    validator = ConfigValidator(args.config_file)
    await validator.validate_and_extract()


if __name__ == "__main__":
    asyncio.run(main())
